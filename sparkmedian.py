import mathimport globimport timeimport pysparkdef median(l):    half = len(l) // 2    for ind,elm in enumerate(l):        l.pop(ind)        l.insert(ind,math.sqrt(math.sqrt(elm)))    l.sort()    if not len(l) % 2:        return (l[half - 1] + l[half]) / 2.0    return pow(l[half],4)fpath='evfiles/2015/01/'datafiles=glob.glob(fpath+'*.txt')print datafiles[1][16:31]events=[]for fname in datafiles:    events.append(fname[16:31])eventlist=list(set(events))print len(eventlist)# event data as freq dictionaryeventdata=[]freqdict=dict()for eventtime in eventlist:    #print eventtime    for file_name in glob.glob(fpath+eventtime+"*.txt"):        fp = open(file_name, "r")        for line in fp:            words = line.split()            tstamp = eventtime+words[0][-7:]            freq = float(words[1])            if tstamp in freqdict:                freqlist = freqdict[tstamp]                freqlist.append(freq)                freqdict[tstamp] = freqlist            else:                freqdict[tstamp] = [freq]        fp.close()    #eventdata.append([eventtime,freqdict])print len(freqdict)eventdata1=freqdict.items()print eventdata1[0]#dataset to RDDeventdataRDD=sc.parallelize(eventdata1,64 )#print eventdataRDD.count()print eventdataRDD.take(1)[0]tic =time.clock()mfreq=(eventdataRDD       .map(lambda (k,v):(k,median(v)))       .sortByKey()       .map(lambda (k,v):(k[:17],(k[-7:],v)))       .groupByKey()       .map(lambda x : (x[0], list(x[1])))       .collect())toc=time.clock()print toc-tic#print mfreqeventdataRDD.unpersist()